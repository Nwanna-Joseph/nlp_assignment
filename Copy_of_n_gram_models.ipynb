{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nwanna-Joseph/nlp_week_1_solution/blob/week3/Copy_of_n_gram_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptgbF38awoyg"
      },
      "source": [
        "\n",
        "<h1 style=\"font-family:verdana;font-size:300%;text-align:center;background-color:#f2f2f2;color:#0d0d0d\">AMMI NLP - Review sessions</h1>\n",
        "\n",
        "<h1 style=\"font-family:verdana;font-size:180%;text-align:Center;color:#993333\"> Lab 3: n-gram models </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGXxtY2-woyi"
      },
      "source": [
        "**Big thanks to Amr Khalifa who improved this lab and made it to a Jupyter Notebook!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E2JQb7PAwoyi"
      },
      "outputs": [],
      "source": [
        "import io, sys, math, re\n",
        "from collections import defaultdict\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm __MACOSX/._train1.txt __MACOSX/._train2.txt __MACOSX/._valid1.txt __MACOSX/._valid2.txt train1.txt train2.txt valid1.txt valid2.txt\n",
        "!wget -O download.zip https://github.com/Nwanna-Joseph/nlp_week_1_solution/blob/week3/train_valid_files.zip?raw=true\n",
        "!unzip download.zip\n",
        "!rm download.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnQc9q_CqvSY",
        "outputId": "605a7b2e-9b83-4cb1-b1ea-919ed2a44f81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '__MACOSX/._train1.txt': No such file or directory\n",
            "rm: cannot remove '__MACOSX/._train2.txt': No such file or directory\n",
            "rm: cannot remove '__MACOSX/._valid1.txt': No such file or directory\n",
            "rm: cannot remove '__MACOSX/._valid2.txt': No such file or directory\n",
            "rm: cannot remove 'train1.txt': No such file or directory\n",
            "rm: cannot remove 'train2.txt': No such file or directory\n",
            "rm: cannot remove 'valid1.txt': No such file or directory\n",
            "rm: cannot remove 'valid2.txt': No such file or directory\n",
            "--2022-05-16 06:23:53--  https://github.com/Nwanna-Joseph/nlp_week_1_solution/blob/week3/train_valid_files.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/Nwanna-Joseph/nlp_week_1_solution/raw/week3/train_valid_files.zip [following]\n",
            "--2022-05-16 06:23:53--  https://github.com/Nwanna-Joseph/nlp_week_1_solution/raw/week3/train_valid_files.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Nwanna-Joseph/nlp_week_1_solution/week3/train_valid_files.zip [following]\n",
            "--2022-05-16 06:23:53--  https://raw.githubusercontent.com/Nwanna-Joseph/nlp_week_1_solution/week3/train_valid_files.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3366065 (3.2M) [application/zip]\n",
            "Saving to: ‘download.zip’\n",
            "\n",
            "download.zip        100%[===================>]   3.21M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-05-16 06:23:54 (243 MB/s) - ‘download.zip’ saved [3366065/3366065]\n",
            "\n",
            "Archive:  download.zip\n",
            "  inflating: valid2.txt              \n",
            "  inflating: __MACOSX/._valid2.txt   \n",
            "  inflating: valid1.txt              \n",
            "  inflating: __MACOSX/._valid1.txt   \n",
            "  inflating: train2.txt              \n",
            "  inflating: __MACOSX/._train2.txt   \n",
            "  inflating: train1.txt              \n",
            "  inflating: __MACOSX/._train1.txt   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vbGmxDGqwoyj"
      },
      "outputs": [],
      "source": [
        "# data_loader\n",
        "def load_data(filename):\n",
        "    '''\n",
        "    parameters:\n",
        "    filename (string): datafile\n",
        "    \n",
        "    Returns:\n",
        "    data (list of lists): each list is a sentence of the text \n",
        "    vocab (dictionary): {word: no of times it appears in the text}\n",
        "    '''\n",
        "    fin = io.open(filename, 'r', encoding='utf-8')\n",
        "    data = []\n",
        "    vocab = defaultdict(lambda:0)\n",
        "    for line in fin:\n",
        "        sentence = line.split()\n",
        "        data.append(sentence)\n",
        "        for word in sentence:\n",
        "            vocab[word] += 1\n",
        "    return data, vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8qH4Xgnswoyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d1bfc7-daa6-4af9-a618-c5372fddc790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load training set..\n",
            "\n",
            "\n",
            "['<s>', 'my', 'fathers', \"don't\", 'speak', 'dutch.', '</s>']\n",
            "\n",
            "\n",
            "how : 107\n",
            "load validation set\n"
          ]
        }
      ],
      "source": [
        "print(\"load training set..\")\n",
        "print(\"\\n\")\n",
        "train_data, vocab = load_data(\"train1.txt\")\n",
        "print(train_data[0])\n",
        "print(\"\\n\")\n",
        "print(\"how :\",vocab['how'])\n",
        "print(\"load validation set\")\n",
        "valid_data, _ = load_data(\"valid1.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY5L6nRk4Uwk",
        "outputId": "84fa2d6e-ee50-4af3-fe66-ab7e3a846981"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5561"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for each_sentence in [['<s>', 'my', 'fathers', \"don't\", 'speak', 'dutch.', '</s>']]:\n",
        "  buffer = []\n",
        "  for position,each_word in enumerate(each_sentence):\n",
        "\n",
        "    n=3\n",
        "\n",
        "    if(len(buffer) >= n):\n",
        "      buffer.pop(0)\n",
        "\n",
        "    print(buffer,each_word)\n",
        "    buffer.append(each_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOzfwCtXnfJc",
        "outputId": "7f821b7d-7b38-4a48-ebf5-0a07cec73fff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[] <s>\n",
            "['<s>'] my\n",
            "['<s>', 'my'] fathers\n",
            "['my', 'fathers'] don't\n",
            "['fathers', \"don't\"] speak\n",
            "[\"don't\", 'speak'] dutch.\n",
            "['speak', 'dutch.'] </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oLjy9Cg9woyk"
      },
      "outputs": [],
      "source": [
        "def remove_rare_words(data, vocab, mincount = 1):\n",
        "    '''\n",
        "    Parameters:\n",
        "    data (list of lists): each list is a sentence of the text \n",
        "    vocab (dictionary): {word: no of times it appears in the text}\n",
        "    mincount(int): the minimum count \n",
        "    \n",
        "    Returns: \n",
        "    data_with_unk(list of lists): data after replacing rare words with <unk> token\n",
        "    '''\n",
        "    # replace words in data that are not in the vocab \n",
        "    # or have a count that is below mincount\n",
        "    data_with_unk = []\n",
        "    print()\n",
        "    ## FILL CODE\n",
        "    for each_sentence in data:\n",
        "      sentence = each_sentence\n",
        "      sentence_copy = []\n",
        "      for each_word in sentence:\n",
        "        word = each_word\n",
        "        word_frequency = vocab.get(word)\n",
        "\n",
        "        if(word_frequency == None):\n",
        "          sentence_copy.append('<unk>')\n",
        "          continue\n",
        "        \n",
        "        if ( word_frequency < mincount):\n",
        "          sentence_copy.append('<unk>')\n",
        "          continue\n",
        "\n",
        "        sentence_copy.append(word)\n",
        "\n",
        "      data_with_unk.append(sentence_copy)\n",
        "\n",
        "    return data_with_unk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oVpaVzzUwoyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b72de342-4382-42bf-a76d-f390cc332f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remove rare words\n",
            "\n",
            "\n",
            "['<s>', 'my', 'fathers', \"don't\", 'speak', 'dutch.', '</s>']\n"
          ]
        }
      ],
      "source": [
        "print(\"remove rare words\")\n",
        "train_data = remove_rare_words(train_data, vocab, mincount = 1)\n",
        "valid_data = remove_rare_words(valid_data, vocab, mincount = 1)\n",
        "#train_data\n",
        "print(train_data[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_q5jZS4Fwoyl"
      },
      "outputs": [],
      "source": [
        "def build_ngram(data, n):\n",
        "    '''\n",
        "    Parameters:\n",
        "    data (list of lists): each list is a sentence of the text \n",
        "    n (int): size of the n-gram\n",
        "    \n",
        "    Returns:\n",
        "    prob (dictionary of dictionary)\n",
        "    {\n",
        "        context: {word:probability of this word given context}\n",
        "    }\n",
        "    '''\n",
        "    total_number_words = 0\n",
        "    counts = {}\n",
        "    word_frequency = {}\n",
        "\n",
        "    for sentence in data:\n",
        "        sentence = tuple(sentence)\n",
        "        \n",
        "        ## FILL CODE\n",
        "        # dict can be indexed by tuples\n",
        "        # store in the same dict all the ngrams\n",
        "        # by using the context as a key and the word as a value\n",
        "\n",
        "        # we'll be using stupid back off\n",
        "\n",
        "        for position,each_word in enumerate(sentence):\n",
        "          word = each_word\n",
        "          word_tuple = tuple([word])\n",
        "          \n",
        "          last_n_words = []\n",
        "          for i in range(n-1): #index from current word to last n chars, n_gram\n",
        "            \n",
        "            last_i_word = position-i-1\n",
        "            if(last_i_word < 0):\n",
        "              continue\n",
        "\n",
        "            last_n_words.insert(0,sentence[last_i_word])\n",
        "\n",
        "            #convert list to tuple and index\n",
        "            last_n_words_tuple = tuple(last_n_words)\n",
        "\n",
        "            if(counts.get(last_n_words_tuple) == None):\n",
        "              counts[last_n_words_tuple] = {}\n",
        "            \n",
        "            word_count = counts.get(last_n_words_tuple,{}).get(word_tuple,0)\n",
        "            \n",
        "            counts[last_n_words_tuple][word_tuple] = word_count + 1\n",
        "\n",
        "            #update frequency lookup\n",
        "            word_frequency[last_n_words_tuple] = word_frequency.get(last_n_words_tuple,0) +1\n",
        "\n",
        "    # print(word_frequency)\n",
        "    # print(word_frequency[tuple(['how'])])\n",
        "    # print(counts[tuple(['<s>'])])\n",
        "    # print(counts)\n",
        "    # return\n",
        "\n",
        "    prob = {}\n",
        "    # Build the probabilities from the counts\n",
        "    # Be careful with how you normalize!\n",
        "\n",
        "    for context in counts.keys():\n",
        "    ## FILL CODE\n",
        "      context_total_appearance = word_frequency[context]\n",
        "\n",
        "      # if counts[context]\n",
        "      \n",
        "      for entry in counts[context]:\n",
        "        if (prob.get(context) == None):\n",
        "          prob[context] = {}\n",
        "        prob[context][entry] = counts[context][entry] / context_total_appearance\n",
        "\n",
        "    # print(prob)\n",
        "\n",
        "    return prob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "['a','b','c','d','e'][2-1:2-2:-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pu14AuXyNGo",
        "outputId": "c34a4591-345c-4451-dd8e-4bf7a6705768"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['b']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuple([1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpfvhm8V-Csf",
        "outputId": "78e48268-33d6-4c24-d6c9-e5e76684ff05"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pAkib8DJwoyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b81f97d-6815-4b28-840d-6394c1168f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build ngram model with n =  3\n"
          ]
        }
      ],
      "source": [
        "# RUN TO BUILD NGRAM MODEL\n",
        "\n",
        "n = 3\n",
        "print(\"build ngram model with n = \", n)\n",
        "# print(train_data[:2])\n",
        "model = build_ngram(train_data[:], n)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "QNs3qlFy9tuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F0Oo84Bwoym"
      },
      "source": [
        "Here, implement a recursive function over shorter and shorter context to compute a \"stupid backoff model\". An interpolation model can also be implemented this way."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[val for val in vocab.values()]"
      ],
      "metadata": {
        "id": "ErmfjL4seFqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KgQz6owswoym"
      },
      "outputs": [],
      "source": [
        "total_words = sum([val for val in vocab.values()])\n",
        "def get_prob(model, context, w):\n",
        "    '''\n",
        "    Parameters: \n",
        "    model (dictionary of dictionary)\n",
        "    {\n",
        "        context: {word:probability of this word given context}\n",
        "    } \n",
        "    context (list of strings): a sentence\n",
        "    w(string): the word we need to find it's probability given the context\n",
        "    \n",
        "    Retunrs:\n",
        "    prob(float): probability of this word given the context \n",
        "    '''\n",
        "\n",
        "    # code a recursive function over \n",
        "    # smaller and smaller context\n",
        "    # to compute the backoff model\n",
        "    \n",
        "    ## FILL CODE\n",
        "\n",
        "    length = len(context)\n",
        "    backoff = 1\n",
        "    word_tuple = tuple([w])\n",
        "    for i in range(len(context)):\n",
        "      i_to_tuple = tuple(context[i:])\n",
        "      # print(i_to_tuple,\"|\",word_tuple)\n",
        "\n",
        "      prob = model.get(i_to_tuple,{}).get(word_tuple)\n",
        "       \n",
        "      if prob == None:\n",
        "        backoff * 0.4\n",
        "      else:\n",
        "        return prob * backoff\n",
        "        \n",
        "    return backoff * (vocab.get(w,0)/total_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_prob(model, ['<s>', 'my', 'fathers', \"don't\", 'speak'], 'dutch.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFAyHLqMagA1",
        "outputId": "e1cf63a3-54e4-4810-be8e-3a7f4a03861a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FU-thbUUwoym"
      },
      "outputs": [],
      "source": [
        "def perplexity(model, data, n):\n",
        "    '''\n",
        "    Parameters: \n",
        "    model (dictionary of dictionary)\n",
        "    {\n",
        "        context: {word:probability of this word given context}\n",
        "    } \n",
        "    data (list of lists): each list is a sentence of the text\n",
        "    n(int): size of the n-gram\n",
        "    \n",
        "    Retunrs:\n",
        "    perp(float): the perplexity of the model \n",
        "    '''\n",
        "\n",
        "    ## FILL CODE\n",
        "    prob = 1\n",
        "    for each_sentence in data:\n",
        "      buffer = []\n",
        "      for position,each_word in enumerate(each_sentence):\n",
        "        if(len(buffer) >= n):\n",
        "          buffer.pop(0)\n",
        "        prob += get_prob(model, buffer, each_word)\n",
        "        buffer.append(each_word)\n",
        "\n",
        "    return prob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_data"
      ],
      "metadata": {
        "id": "yKTnRf9uBEI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FGh0-J6Iwoym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0c189cc-0af0-449d-e01a-fa29bc540c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The perplexity is 138.72070613113132\n"
          ]
        }
      ],
      "source": [
        "# COMPUTE PERPLEXITY ON VALIDATION SET\n",
        "\n",
        "print(\"The perplexity is\", perplexity(model, valid_data, n=n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7XEoMNbvwoyn"
      },
      "outputs": [],
      "source": [
        "def get_proba_distrib(model, context):\n",
        "    ## need to get the the words after the context and their probability of appearance\n",
        "    ## after this context \n",
        "    '''\n",
        "    Parameters: \n",
        "    model (dictionary of dictionary)\n",
        "    {\n",
        "        context: {word:probability of this word given context}\n",
        "    }\n",
        "    context (list of strings): the sentence we need to find the words after it and \n",
        "    thier probabilites\n",
        "    \n",
        "    Retunrs:\n",
        "    words_and_probs(dic): {word: probability of word given context}\n",
        "    \n",
        "    '''\n",
        "    # code a recursive function over context\n",
        "    # to find the longest available ngram\n",
        "    \n",
        "    ## FILL CODE\n",
        "\n",
        "    for _ in range(len(context)):\n",
        "      #check\n",
        "      map = model[tuple([context])]\n",
        "      if map == None:\n",
        "        #no match, remove pop(0)\n",
        "        context.pop(0)\n",
        "        #retry\n",
        "        continue\n",
        "      \n",
        "      #match, return nexts...\n",
        "      return map"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "['1','2','3'][-1]"
      ],
      "metadata": {
        "id": "iu-efwtiJb5O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "49a8b08c-add7-4ab9-cf66-fe9da61a919a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rr = ['1']\n",
        "for _ in range(len(rr)):\n",
        "  print(rr)\n",
        "  rr.pop(0)\n",
        "print(rr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOZ8T_l1vswu",
        "outputId": "abda44ae-351f-4671-f163-a11bcd65b21f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1']\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "KMlF6uMHwoyn"
      },
      "outputs": [],
      "source": [
        "def generate(model):\n",
        "    '''\n",
        "    Parameters: \n",
        "    model (dictionary of dictionary)\n",
        "    {\n",
        "        context: {word:probability of this word given context}\n",
        "    }\n",
        "    \n",
        "    Retunrs:\n",
        "    sentence (list of strings): a sentence sampled according to the language model. \n",
        "    \n",
        "\n",
        "    '''\n",
        "    # generate a sentence. A sentence starts with a <s> and ends with a </s>\n",
        "    # Possiblly a use function is:\n",
        "    # np.random.choice(x, 1, p = y)\n",
        "\n",
        "    # where x is a list of things to sample from\n",
        "    # and y is a list of probability (of the same length as x)\n",
        "    sentence = [\"<s>\"]\n",
        "    while sentence[-1] != \"</s>\" and len(sentence)<100:\n",
        "        context = sentence[-1]\n",
        "        next_choice_of_words = get_proba_distrib(model,context)\n",
        "        next_choice_of_words_lists = [k[0] for k in next_choice_of_words.keys()]\n",
        "        # print(next_choice_of_words_lists)\n",
        "        # print(next_choice_of_words)\n",
        "        choice = np.random.choice(next_choice_of_words_lists, 1, replace=False)\n",
        "        sentence.append(choice[0])\n",
        "        \n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "iWTH570vwoyn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b802435f-007d-4abc-9d39-ed7de41456d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>\n",
            "lady\n",
            "was\n",
            "haunted.\n",
            "Generated sentence:  ['<s>', 'lady', 'was', 'haunted.', '</s>']\n"
          ]
        }
      ],
      "source": [
        "# GENERATE A SENTENCE FROM THE MODEL\n",
        "\n",
        "\n",
        "print(\"Generated sentence: \",generate(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXY_E70mwoyn"
      },
      "source": [
        "Once you are done implementing the model, evaluation and generation code, you can try changing the value of `n`, and play with a larger training set (`train2.txt` and `valid2.txt`). You can also try to implement an interpolation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqSNFVsYwoyn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2+"
    },
    "colab": {
      "name": "Copy of n_gram_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}