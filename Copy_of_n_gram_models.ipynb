{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nwanna-Joseph/nlp_week_1_solution/blob/week3/Copy_of_n_gram_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptgbF38awoyg"
      },
      "source": [
        "\n",
        "<h1 style=\"font-family:verdana;font-size:300%;text-align:center;background-color:#f2f2f2;color:#0d0d0d\">AMMI NLP - Review sessions</h1>\n",
        "\n",
        "<h1 style=\"font-family:verdana;font-size:180%;text-align:Center;color:#993333\"> Lab 3: n-gram models </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGXxtY2-woyi"
      },
      "source": [
        "**Big thanks to Amr Khalifa who improved this lab and made it to a Jupyter Notebook!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E2JQb7PAwoyi"
      },
      "outputs": [],
      "source": [
        "import io, sys, math, re\n",
        "from collections import defaultdict\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm __MACOSX/._train1.txt __MACOSX/._train2.txt __MACOSX/._valid1.txt __MACOSX/._valid2.txt train1.txt train2.txt valid1.txt valid2.txt\n",
        "!wget -O download.zip https://github.com/Nwanna-Joseph/nlp_week_1_solution/blob/week3/train_valid_files.zip?raw=true\n",
        "!unzip download.zip\n",
        "!rm download.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnQc9q_CqvSY",
        "outputId": "605a7b2e-9b83-4cb1-b1ea-919ed2a44f81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '__MACOSX/._train1.txt': No such file or directory\n",
            "rm: cannot remove '__MACOSX/._train2.txt': No such file or directory\n",
            "rm: cannot remove '__MACOSX/._valid1.txt': No such file or directory\n",
            "rm: cannot remove '__MACOSX/._valid2.txt': No such file or directory\n",
            "rm: cannot remove 'train1.txt': No such file or directory\n",
            "rm: cannot remove 'train2.txt': No such file or directory\n",
            "rm: cannot remove 'valid1.txt': No such file or directory\n",
            "rm: cannot remove 'valid2.txt': No such file or directory\n",
            "--2022-05-16 06:23:53--  https://github.com/Nwanna-Joseph/nlp_week_1_solution/blob/week3/train_valid_files.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/Nwanna-Joseph/nlp_week_1_solution/raw/week3/train_valid_files.zip [following]\n",
            "--2022-05-16 06:23:53--  https://github.com/Nwanna-Joseph/nlp_week_1_solution/raw/week3/train_valid_files.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Nwanna-Joseph/nlp_week_1_solution/week3/train_valid_files.zip [following]\n",
            "--2022-05-16 06:23:53--  https://raw.githubusercontent.com/Nwanna-Joseph/nlp_week_1_solution/week3/train_valid_files.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3366065 (3.2M) [application/zip]\n",
            "Saving to: ‘download.zip’\n",
            "\n",
            "download.zip        100%[===================>]   3.21M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-05-16 06:23:54 (243 MB/s) - ‘download.zip’ saved [3366065/3366065]\n",
            "\n",
            "Archive:  download.zip\n",
            "  inflating: valid2.txt              \n",
            "  inflating: __MACOSX/._valid2.txt   \n",
            "  inflating: valid1.txt              \n",
            "  inflating: __MACOSX/._valid1.txt   \n",
            "  inflating: train2.txt              \n",
            "  inflating: __MACOSX/._train2.txt   \n",
            "  inflating: train1.txt              \n",
            "  inflating: __MACOSX/._train1.txt   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vbGmxDGqwoyj"
      },
      "outputs": [],
      "source": [
        "# data_loader\n",
        "def load_data(filename):\n",
        "    '''\n",
        "    parameters:\n",
        "    filename (string): datafile\n",
        "    \n",
        "    Returns:\n",
        "    data (list of lists): each list is a sentence of the text \n",
        "    vocab (dictionary): {word: no of times it appears in the text}\n",
        "    '''\n",
        "    fin = io.open(filename, 'r', encoding='utf-8')\n",
        "    data = []\n",
        "    vocab = defaultdict(lambda:0)\n",
        "    for line in fin:\n",
        "        sentence = line.split()\n",
        "        data.append(sentence)\n",
        "        for word in sentence:\n",
        "            vocab[word] += 1\n",
        "    return data, vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8qH4Xgnswoyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d1bfc7-daa6-4af9-a618-c5372fddc790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load training set..\n",
            "\n",
            "\n",
            "['<s>', 'my', 'fathers', \"don't\", 'speak', 'dutch.', '</s>']\n",
            "\n",
            "\n",
            "how : 107\n",
            "load validation set\n"
          ]
        }
      ],
      "source": [
        "print(\"load training set..\")\n",
        "print(\"\\n\")\n",
        "train_data, vocab = load_data(\"train1.txt\")\n",
        "print(train_data[0])\n",
        "print(\"\\n\")\n",
        "print(\"how :\",vocab['how'])\n",
        "print(\"load validation set\")\n",
        "valid_data, _ = load_data(\"valid1.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY5L6nRk4Uwk",
        "outputId": "84fa2d6e-ee50-4af3-fe66-ab7e3a846981"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5561"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for each_sentence in [['<s>', 'my', 'fathers', \"don't\", 'speak', 'dutch.', '</s>']]:\n",
        "  buffer = []\n",
        "  for position,each_word in enumerate(each_sentence):\n",
        "\n",
        "    n=3\n",
        "\n",
        "    if(len(buffer) >= n):\n",
        "      buffer.pop(0)\n",
        "\n",
        "    print(buffer,each_word)\n",
        "    buffer.append(each_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOzfwCtXnfJc",
        "outputId": "7f821b7d-7b38-4a48-ebf5-0a07cec73fff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[] <s>\n",
            "['<s>'] my\n",
            "['<s>', 'my'] fathers\n",
            "['my', 'fathers'] don't\n",
            "['fathers', \"don't\"] speak\n",
            "[\"don't\", 'speak'] dutch.\n",
            "['speak', 'dutch.'] </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oLjy9Cg9woyk"
      },
      "outputs": [],
      "source": [
        "def remove_rare_words(data, vocab, mincount = 1):\n",
        "    '''\n",
        "    Parameters:\n",
        "    data (list of lists): each list is a sentence of the text \n",
        "    vocab (dictionary): {word: no of times it appears in the text}\n",
        "    mincount(int): the minimum count \n",
        "    \n",
        "    Returns: \n",
        "    data_with_unk(list of lists): data after replacing rare words with <unk> token\n",
        "    '''\n",
        "    # replace words in data that are not in the vocab \n",
        "    # or have a count that is below mincount\n",
        "    data_with_unk = []\n",
        "    print()\n",
        "    ## FILL CODE\n",
        "    for each_sentence in data:\n",
        "      sentence = each_sentence\n",
        "      sentence_copy = []\n",
        "      for each_word in sentence:\n",
        "        word = each_word\n",
        "        word_frequency = vocab.get(word)\n",
        "\n",
        "        if(word_frequency == None):\n",
        "          sentence_copy.append('<unk>')\n",
        "          continue\n",
        "        \n",
        "        if ( word_frequency < mincount):\n",
        "          sentence_copy.append('<unk>')\n",
        "          continue\n",
        "\n",
        "        sentence_copy.append(word)\n",
        "\n",
        "      data_with_unk.append(sentence_copy)\n",
        "\n",
        "    return data_with_unk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oVpaVzzUwoyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b72de342-4382-42bf-a76d-f390cc332f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remove rare words\n",
            "\n",
            "\n",
            "['<s>', 'my', 'fathers', \"don't\", 'speak', 'dutch.', '</s>']\n"
          ]
        }
      ],
      "source": [
        "print(\"remove rare words\")\n",
        "train_data = remove_rare_words(train_data, vocab, mincount = 1)\n",
        "valid_data = remove_rare_words(valid_data, vocab, mincount = 1)\n",
        "#train_data\n",
        "print(train_data[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_q5jZS4Fwoyl"
      },
      "outputs": [],
      "source": [
        "def build_ngram(data, n):\n",
        "    '''\n",
        "    Parameters:\n",
        "    data (list of lists): each list is a sentence of the text \n",
        "    n (int): size of the n-gram\n",
        "    \n",
        "    Returns:\n",
        "    prob (dictionary of dictionary)\n",
        "    {\n",
        "        context: {word:probability of this word given context}\n",
        "    }\n",
        "    '''\n",
        "    total_number_words = 0\n",
        "    counts = {}\n",
        "    word_frequency = {}\n",
        "\n",
        "    for sentence in data:\n",
        "        sentence = tuple(sentence)\n",
        "        \n",
        "        ## FILL CODE\n",
        "        # dict can be indexed by tuples\n",
        "        # store in the same dict all the ngrams\n",
        "        # by using the context as a key and the word as a value\n",
        "\n",
        "        # we'll be using stupid back off\n",
        "\n",
        "        for position,each_word in enumerate(sentence):\n",
        "          word = each_word\n",
        "          word_tuple = tuple([word])\n",
        "          \n",
        "          last_n_words = []\n",
        "          for i in range(n-1): #index from current word to last n chars, n_gram\n",
        "            \n",
        "            last_i_word = position-i-1\n",
        "            if(last_i_word < 0):\n",
        "              continue\n",
        "\n",
        "            last_n_words.insert(0,sentence[last_i_word])\n",
        "\n",
        "            #convert list to tuple and index\n",
        "            last_n_words_tuple = tuple(last_n_words)\n",
        "\n",
        "            if(counts.get(last_n_words_tuple) == None):\n",
        "              counts[last_n_words_tuple] = {}\n",
        "            \n",
        "            word_count = counts.get(last_n_words_tuple,{}).get(word_tuple,0)\n",
        "            \n",
        "            counts[last_n_words_tuple][word_tuple] = word_count + 1\n",
        "\n",
        "            #update frequency lookup\n",
        "            word_frequency[last_n_words_tuple] = word_frequency.get(last_n_words_tuple,0) +1\n",
        "\n",
        "    # print(word_frequency)\n",
        "    # print(word_frequency[tuple(['how'])])\n",
        "    # print(counts[tuple(['<s>'])])\n",
        "    # print(counts)\n",
        "    # return\n",
        "\n",
        "    prob = {}\n",
        "    # Build the probabilities from the counts\n",
        "    # Be careful with how you normalize!\n",
        "\n",
        "    for context in counts.keys():\n",
        "    ## FILL CODE\n",
        "      context_total_appearance = word_frequency[context]\n",
        "\n",
        "      # if counts[context]\n",
        "      \n",
        "      for entry in counts[context]:\n",
        "        if (prob.get(context) == None):\n",
        "          prob[context] = {}\n",
        "        prob[context][entry] = counts[context][entry] / context_total_appearance\n",
        "\n",
        "    # print(prob)\n",
        "\n",
        "    return prob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "['a','b','c','d','e'][2-1:2-2:-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pu14AuXyNGo",
        "outputId": "c34a4591-345c-4451-dd8e-4bf7a6705768"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['b']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuple([1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpfvhm8V-Csf",
        "outputId": "78e48268-33d6-4c24-d6c9-e5e76684ff05"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pAkib8DJwoyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b81f97d-6815-4b28-840d-6394c1168f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build ngram model with n =  3\n"
          ]
        }
      ],
      "source": [
        "# RUN TO BUILD NGRAM MODEL\n",
        "\n",
        "n = 3\n",
        "print(\"build ngram model with n = \", n)\n",
        "# print(train_data[:2])\n",
        "model = build_ngram(train_data[:], n)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "QNs3qlFy9tuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F0Oo84Bwoym"
      },
      "source": [
        "Here, implement a recursive function over shorter and shorter context to compute a \"stupid backoff model\". An interpolation model can also be implemented this way."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[val for val in vocab.values()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErmfjL4seFqO",
        "outputId": "298837d4-fbba-43c4-8ea9-33d7080ad13b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5561,\n",
              " 203,\n",
              " 1,\n",
              " 199,\n",
              " 12,\n",
              " 1,\n",
              " 5561,\n",
              " 4,\n",
              " 735,\n",
              " 4669,\n",
              " 3,\n",
              " 430,\n",
              " 5,\n",
              " 16,\n",
              " 1761,\n",
              " 44,\n",
              " 246,\n",
              " 6,\n",
              " 2,\n",
              " 258,\n",
              " 12,\n",
              " 1478,\n",
              " 104,\n",
              " 26,\n",
              " 900,\n",
              " 21,\n",
              " 24,\n",
              " 23,\n",
              " 980,\n",
              " 1,\n",
              " 72,\n",
              " 36,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 222,\n",
              " 507,\n",
              " 10,\n",
              " 25,\n",
              " 1,\n",
              " 70,\n",
              " 7,\n",
              " 62,\n",
              " 11,\n",
              " 329,\n",
              " 356,\n",
              " 22,\n",
              " 1103,\n",
              " 788,\n",
              " 283,\n",
              " 835,\n",
              " 192,\n",
              " 15,\n",
              " 6,\n",
              " 14,\n",
              " 582,\n",
              " 274,\n",
              " 6,\n",
              " 351,\n",
              " 3,\n",
              " 127,\n",
              " 1045,\n",
              " 35,\n",
              " 3,\n",
              " 2,\n",
              " 17,\n",
              " 6,\n",
              " 36,\n",
              " 107,\n",
              " 166,\n",
              " 75,\n",
              " 16,\n",
              " 1,\n",
              " 59,\n",
              " 266,\n",
              " 27,\n",
              " 758,\n",
              " 166,\n",
              " 45,\n",
              " 6,\n",
              " 18,\n",
              " 1,\n",
              " 1,\n",
              " 45,\n",
              " 2,\n",
              " 155,\n",
              " 9,\n",
              " 6,\n",
              " 1,\n",
              " 19,\n",
              " 80,\n",
              " 692,\n",
              " 3,\n",
              " 77,\n",
              " 3,\n",
              " 2,\n",
              " 47,\n",
              " 2,\n",
              " 2,\n",
              " 10,\n",
              " 299,\n",
              " 2,\n",
              " 190,\n",
              " 167,\n",
              " 5,\n",
              " 119,\n",
              " 195,\n",
              " 54,\n",
              " 110,\n",
              " 10,\n",
              " 102,\n",
              " 6,\n",
              " 33,\n",
              " 14,\n",
              " 220,\n",
              " 1,\n",
              " 1,\n",
              " 24,\n",
              " 45,\n",
              " 15,\n",
              " 6,\n",
              " 174,\n",
              " 2,\n",
              " 1,\n",
              " 47,\n",
              " 145,\n",
              " 48,\n",
              " 19,\n",
              " 22,\n",
              " 3,\n",
              " 3,\n",
              " 129,\n",
              " 142,\n",
              " 53,\n",
              " 33,\n",
              " 76,\n",
              " 6,\n",
              " 18,\n",
              " 38,\n",
              " 8,\n",
              " 5,\n",
              " 2,\n",
              " 77,\n",
              " 4,\n",
              " 112,\n",
              " 3,\n",
              " 58,\n",
              " 7,\n",
              " 3,\n",
              " 5,\n",
              " 2,\n",
              " 40,\n",
              " 4,\n",
              " 1,\n",
              " 220,\n",
              " 1,\n",
              " 1,\n",
              " 4,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 63,\n",
              " 3,\n",
              " 3,\n",
              " 1,\n",
              " 226,\n",
              " 2,\n",
              " 4,\n",
              " 44,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 53,\n",
              " 464,\n",
              " 211,\n",
              " 217,\n",
              " 5,\n",
              " 6,\n",
              " 13,\n",
              " 7,\n",
              " 2,\n",
              " 1,\n",
              " 7,\n",
              " 8,\n",
              " 2,\n",
              " 114,\n",
              " 1,\n",
              " 43,\n",
              " 51,\n",
              " 112,\n",
              " 28,\n",
              " 3,\n",
              " 2,\n",
              " 8,\n",
              " 44,\n",
              " 126,\n",
              " 1,\n",
              " 6,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 4,\n",
              " 16,\n",
              " 15,\n",
              " 1,\n",
              " 11,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 209,\n",
              " 5,\n",
              " 1,\n",
              " 48,\n",
              " 6,\n",
              " 16,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 35,\n",
              " 179,\n",
              " 4,\n",
              " 1,\n",
              " 57,\n",
              " 45,\n",
              " 25,\n",
              " 27,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 11,\n",
              " 1,\n",
              " 2,\n",
              " 98,\n",
              " 3,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 11,\n",
              " 12,\n",
              " 1,\n",
              " 2,\n",
              " 19,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 21,\n",
              " 13,\n",
              " 35,\n",
              " 2,\n",
              " 53,\n",
              " 72,\n",
              " 8,\n",
              " 2,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 11,\n",
              " 4,\n",
              " 34,\n",
              " 65,\n",
              " 18,\n",
              " 5,\n",
              " 65,\n",
              " 1,\n",
              " 1,\n",
              " 13,\n",
              " 1,\n",
              " 47,\n",
              " 4,\n",
              " 8,\n",
              " 5,\n",
              " 10,\n",
              " 12,\n",
              " 3,\n",
              " 16,\n",
              " 2,\n",
              " 3,\n",
              " 10,\n",
              " 38,\n",
              " 8,\n",
              " 11,\n",
              " 7,\n",
              " 3,\n",
              " 7,\n",
              " 143,\n",
              " 9,\n",
              " 2,\n",
              " 1,\n",
              " 7,\n",
              " 107,\n",
              " 29,\n",
              " 2,\n",
              " 45,\n",
              " 3,\n",
              " 56,\n",
              " 3,\n",
              " 9,\n",
              " 4,\n",
              " 33,\n",
              " 234,\n",
              " 1,\n",
              " 65,\n",
              " 131,\n",
              " 3,\n",
              " 16,\n",
              " 6,\n",
              " 7,\n",
              " 2,\n",
              " 10,\n",
              " 160,\n",
              " 6,\n",
              " 1,\n",
              " 37,\n",
              " 35,\n",
              " 29,\n",
              " 29,\n",
              " 128,\n",
              " 1,\n",
              " 110,\n",
              " 209,\n",
              " 9,\n",
              " 114,\n",
              " 116,\n",
              " 2,\n",
              " 9,\n",
              " 3,\n",
              " 144,\n",
              " 29,\n",
              " 2,\n",
              " 2,\n",
              " 4,\n",
              " 8,\n",
              " 64,\n",
              " 3,\n",
              " 24,\n",
              " 14,\n",
              " 2,\n",
              " 1,\n",
              " 43,\n",
              " 14,\n",
              " 3,\n",
              " 25,\n",
              " 42,\n",
              " 8,\n",
              " 1,\n",
              " 2,\n",
              " 5,\n",
              " 11,\n",
              " 59,\n",
              " 28,\n",
              " 20,\n",
              " 5,\n",
              " 23,\n",
              " 25,\n",
              " 15,\n",
              " 3,\n",
              " 15,\n",
              " 15,\n",
              " 15,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 54,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 174,\n",
              " 10,\n",
              " 1,\n",
              " 7,\n",
              " 3,\n",
              " 36,\n",
              " 2,\n",
              " 122,\n",
              " 6,\n",
              " 5,\n",
              " 5,\n",
              " 2,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 3,\n",
              " 12,\n",
              " 16,\n",
              " 2,\n",
              " 36,\n",
              " 101,\n",
              " 44,\n",
              " 3,\n",
              " 10,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 4,\n",
              " 2,\n",
              " 1,\n",
              " 6,\n",
              " 1,\n",
              " 4,\n",
              " 115,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 38,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 24,\n",
              " 3,\n",
              " 2,\n",
              " 1,\n",
              " 10,\n",
              " 12,\n",
              " 34,\n",
              " 3,\n",
              " 7,\n",
              " 2,\n",
              " 36,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 42,\n",
              " 10,\n",
              " 1,\n",
              " 6,\n",
              " 58,\n",
              " 2,\n",
              " 1,\n",
              " 140,\n",
              " 105,\n",
              " 2,\n",
              " 52,\n",
              " 7,\n",
              " 31,\n",
              " 4,\n",
              " 1,\n",
              " 9,\n",
              " 10,\n",
              " 1,\n",
              " 13,\n",
              " 23,\n",
              " 1,\n",
              " 12,\n",
              " 10,\n",
              " 79,\n",
              " 1,\n",
              " 5,\n",
              " 16,\n",
              " 11,\n",
              " 22,\n",
              " 185,\n",
              " 2,\n",
              " 1,\n",
              " 77,\n",
              " 1,\n",
              " 11,\n",
              " 31,\n",
              " 4,\n",
              " 13,\n",
              " 45,\n",
              " 4,\n",
              " 3,\n",
              " 1,\n",
              " 4,\n",
              " 5,\n",
              " 72,\n",
              " 30,\n",
              " 4,\n",
              " 4,\n",
              " 1,\n",
              " 119,\n",
              " 3,\n",
              " 1,\n",
              " 7,\n",
              " 2,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 54,\n",
              " 2,\n",
              " 2,\n",
              " 9,\n",
              " 1,\n",
              " 29,\n",
              " 18,\n",
              " 9,\n",
              " 2,\n",
              " 4,\n",
              " 9,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 13,\n",
              " 4,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 7,\n",
              " 57,\n",
              " 1,\n",
              " 1,\n",
              " 37,\n",
              " 10,\n",
              " 4,\n",
              " 2,\n",
              " 3,\n",
              " 11,\n",
              " 3,\n",
              " 1,\n",
              " 15,\n",
              " 13,\n",
              " 1,\n",
              " 10,\n",
              " 9,\n",
              " 27,\n",
              " 1,\n",
              " 13,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 76,\n",
              " 56,\n",
              " 8,\n",
              " 42,\n",
              " 17,\n",
              " 1,\n",
              " 2,\n",
              " 18,\n",
              " 3,\n",
              " 13,\n",
              " 19,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 30,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 25,\n",
              " 9,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 8,\n",
              " 1,\n",
              " 2,\n",
              " 36,\n",
              " 4,\n",
              " 16,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 14,\n",
              " 13,\n",
              " 10,\n",
              " 6,\n",
              " 7,\n",
              " 19,\n",
              " 9,\n",
              " 30,\n",
              " 2,\n",
              " 1,\n",
              " 41,\n",
              " 18,\n",
              " 10,\n",
              " 5,\n",
              " 3,\n",
              " 5,\n",
              " 3,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 6,\n",
              " 4,\n",
              " 1,\n",
              " 13,\n",
              " 12,\n",
              " 2,\n",
              " 1,\n",
              " 3,\n",
              " 11,\n",
              " 9,\n",
              " 5,\n",
              " 6,\n",
              " 1,\n",
              " 2,\n",
              " 71,\n",
              " 70,\n",
              " 10,\n",
              " 4,\n",
              " 69,\n",
              " 16,\n",
              " 8,\n",
              " 23,\n",
              " 5,\n",
              " 18,\n",
              " 8,\n",
              " 2,\n",
              " 3,\n",
              " 40,\n",
              " 28,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 6,\n",
              " 4,\n",
              " 4,\n",
              " 26,\n",
              " 3,\n",
              " 53,\n",
              " 3,\n",
              " 1,\n",
              " 74,\n",
              " 21,\n",
              " 50,\n",
              " 34,\n",
              " 22,\n",
              " 1,\n",
              " 12,\n",
              " 12,\n",
              " 41,\n",
              " 26,\n",
              " 3,\n",
              " 38,\n",
              " 75,\n",
              " 2,\n",
              " 2,\n",
              " 3,\n",
              " 5,\n",
              " 70,\n",
              " 1,\n",
              " 22,\n",
              " 138,\n",
              " 7,\n",
              " 15,\n",
              " 2,\n",
              " 17,\n",
              " 3,\n",
              " 1,\n",
              " 39,\n",
              " 3,\n",
              " 11,\n",
              " 1,\n",
              " 6,\n",
              " 9,\n",
              " 1,\n",
              " 3,\n",
              " 15,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 5,\n",
              " 17,\n",
              " 5,\n",
              " 6,\n",
              " 6,\n",
              " 4,\n",
              " 3,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 5,\n",
              " 20,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 4,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 13,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 3,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 3,\n",
              " 2,\n",
              " 11,\n",
              " 1,\n",
              " 4,\n",
              " 9,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 30,\n",
              " 1,\n",
              " 9,\n",
              " 20,\n",
              " 3,\n",
              " 22,\n",
              " 46,\n",
              " 3,\n",
              " 11,\n",
              " 1,\n",
              " 1,\n",
              " 25,\n",
              " 1,\n",
              " 11,\n",
              " 2,\n",
              " 1,\n",
              " 7,\n",
              " 4,\n",
              " 1,\n",
              " 4,\n",
              " 4,\n",
              " 36,\n",
              " 4,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 9,\n",
              " 14,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 22,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 17,\n",
              " 26,\n",
              " 4,\n",
              " 8,\n",
              " 5,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 6,\n",
              " 24,\n",
              " 17,\n",
              " 13,\n",
              " 6,\n",
              " 1,\n",
              " 1,\n",
              " 10,\n",
              " 3,\n",
              " 2,\n",
              " 1,\n",
              " 39,\n",
              " 11,\n",
              " 3,\n",
              " 20,\n",
              " 5,\n",
              " 2,\n",
              " 9,\n",
              " 3,\n",
              " 3,\n",
              " 5,\n",
              " 3,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 61,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 67,\n",
              " 3,\n",
              " 1,\n",
              " 41,\n",
              " 7,\n",
              " 11,\n",
              " 2,\n",
              " 4,\n",
              " 10,\n",
              " 2,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 4,\n",
              " 5,\n",
              " 2,\n",
              " 1,\n",
              " 6,\n",
              " 11,\n",
              " 13,\n",
              " 5,\n",
              " 4,\n",
              " 12,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 6,\n",
              " 22,\n",
              " 13,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 20,\n",
              " 14,\n",
              " 4,\n",
              " 31,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 102,\n",
              " 1,\n",
              " 1,\n",
              " 9,\n",
              " 6,\n",
              " 5,\n",
              " 1,\n",
              " 9,\n",
              " 37,\n",
              " 3,\n",
              " 22,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 1,\n",
              " 8,\n",
              " 14,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 21,\n",
              " 2,\n",
              " 22,\n",
              " 4,\n",
              " 1,\n",
              " 4,\n",
              " 12,\n",
              " 35,\n",
              " 1,\n",
              " 20,\n",
              " 26,\n",
              " 5,\n",
              " 2,\n",
              " 1,\n",
              " 19,\n",
              " 7,\n",
              " 8,\n",
              " 4,\n",
              " 30,\n",
              " 2,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 3,\n",
              " 7,\n",
              " 2,\n",
              " 1,\n",
              " 1,\n",
              " 13,\n",
              " 28,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 5,\n",
              " 17,\n",
              " 5,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 2,\n",
              " 4,\n",
              " 1,\n",
              " 6,\n",
              " 1,\n",
              " 23,\n",
              " 33,\n",
              " 10,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 1,\n",
              " 5,\n",
              " 9,\n",
              " 1,\n",
              " 5,\n",
              " 6,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 7,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 6,\n",
              " 1,\n",
              " 37,\n",
              " 41,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KgQz6owswoym"
      },
      "outputs": [],
      "source": [
        "total_words = sum([val for val in vocab.values()])\n",
        "def get_prob(model, context, w):\n",
        "    '''\n",
        "    Parameters: \n",
        "    model (dictionary of dictionary)\n",
        "    {\n",
        "        context: {word:probability of this word given context}\n",
        "    } \n",
        "    context (list of strings): a sentence\n",
        "    w(string): the word we need to find it's probability given the context\n",
        "    \n",
        "    Retunrs:\n",
        "    prob(float): probability of this word given the context \n",
        "    '''\n",
        "\n",
        "    # code a recursive function over \n",
        "    # smaller and smaller context\n",
        "    # to compute the backoff model\n",
        "    \n",
        "    ## FILL CODE\n",
        "\n",
        "    length = len(context)\n",
        "    backoff = 1\n",
        "    word_tuple = tuple([w])\n",
        "    for i in range(len(context)):\n",
        "      i_to_tuple = tuple(context[i:])\n",
        "      # print(i_to_tuple,\"|\",word_tuple)\n",
        "\n",
        "      prob = model.get(i_to_tuple,{}).get(word_tuple)\n",
        "       \n",
        "      if prob == None:\n",
        "        backoff * 0.4\n",
        "      else:\n",
        "        return prob * backoff\n",
        "        \n",
        "    return backoff * (vocab.get(w,0)/total_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_prob(model, ['<s>', 'my', 'fathers', \"don't\", 'speak'], 'dutch.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFAyHLqMagA1",
        "outputId": "e1cf63a3-54e4-4810-be8e-3a7f4a03861a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FU-thbUUwoym"
      },
      "outputs": [],
      "source": [
        "def perplexity(model, data, n):\n",
        "    '''\n",
        "    Parameters: \n",
        "    model (dictionary of dictionary)\n",
        "    {\n",
        "        context: {word:probability of this word given context}\n",
        "    } \n",
        "    data (list of lists): each list is a sentence of the text\n",
        "    n(int): size of the n-gram\n",
        "    \n",
        "    Retunrs:\n",
        "    perp(float): the perplexity of the model \n",
        "    '''\n",
        "\n",
        "    ## FILL CODE\n",
        "    prob = 1\n",
        "    for each_sentence in data:\n",
        "      buffer = []\n",
        "      for position,each_word in enumerate(each_sentence):\n",
        "        if(len(buffer) >= n):\n",
        "          buffer.pop(0)\n",
        "        prob += get_prob(model, buffer, each_word)\n",
        "        buffer.append(each_word)\n",
        "\n",
        "    return prob"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_data"
      ],
      "metadata": {
        "id": "yKTnRf9uBEI2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9958a5e7-3065-444e-ff9a-8b478cd59ee7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<s>',\n",
              "  'we',\n",
              "  '<unk>',\n",
              "  'over',\n",
              "  'the',\n",
              "  'decision',\n",
              "  'to',\n",
              "  'do',\n",
              "  'that.',\n",
              "  '</s>'],\n",
              " ['<s>', 'sami', 'should', 'be', 'aware', 'of', 'the', 'situation.', '</s>'],\n",
              " ['<s>', 'my', 'father', \"won't\", 'allow', 'that.', '</s>'],\n",
              " ['<s>',\n",
              "  'tom',\n",
              "  'goes',\n",
              "  'to',\n",
              "  'the',\n",
              "  'gym',\n",
              "  'three',\n",
              "  'times',\n",
              "  'a',\n",
              "  'week.',\n",
              "  '</s>'],\n",
              " ['<s>', 'ich', '<unk>', '<unk>', '<unk>', '</s>'],\n",
              " ['<s>',\n",
              "  'his',\n",
              "  'speech',\n",
              "  'was',\n",
              "  'an',\n",
              "  'effective',\n",
              "  '<unk>',\n",
              "  'for',\n",
              "  'the',\n",
              "  \"government's\",\n",
              "  '<unk>',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  \"weren't\",\n",
              "  'you',\n",
              "  'the',\n",
              "  'one',\n",
              "  'who',\n",
              "  'was',\n",
              "  'supposed',\n",
              "  'to',\n",
              "  'tell',\n",
              "  'tom',\n",
              "  'and',\n",
              "  'mary',\n",
              "  'they',\n",
              "  \"wouldn't\",\n",
              "  'be',\n",
              "  'allowed',\n",
              "  'to',\n",
              "  'do',\n",
              "  'that?',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  '\"why',\n",
              "  'do',\n",
              "  'you',\n",
              "  'rob',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  'that’s',\n",
              "  'where',\n",
              "  'the',\n",
              "  'money',\n",
              "  '<unk>',\n",
              "  '</s>'],\n",
              " ['<s>', 'are', 'these', 'children', 'yours?', '</s>'],\n",
              " ['<s>', 'this', 'park', \"isn't\", 'open', 'to', 'the', '<unk>', '</s>'],\n",
              " ['<s>',\n",
              "  'sami',\n",
              "  'brought',\n",
              "  'so',\n",
              "  'much',\n",
              "  '<unk>',\n",
              "  'to',\n",
              "  'the',\n",
              "  'work',\n",
              "  'environment.',\n",
              "  '</s>'],\n",
              " ['<s>', 'i', 'need', 'you', 'to', 'sign', 'these', '<unk>', '</s>'],\n",
              " ['<s>',\n",
              "  'his',\n",
              "  'age,',\n",
              "  'she',\n",
              "  'thought,',\n",
              "  'would',\n",
              "  'be',\n",
              "  'rather',\n",
              "  'over',\n",
              "  'thirty',\n",
              "  'than',\n",
              "  'under',\n",
              "  'it.',\n",
              "  '</s>'],\n",
              " ['<s>', 'tom', 'and', 'mary', 'are', '<unk>', \"aren't\", 'they?', '</s>'],\n",
              " ['<s>', 'tom', 'took', 'the', '<unk>', 'away', 'from', 'mary.', '</s>'],\n",
              " ['<s>',\n",
              "  'you',\n",
              "  'mean',\n",
              "  'the',\n",
              "  'world',\n",
              "  'to',\n",
              "  'me,',\n",
              "  'you',\n",
              "  'really',\n",
              "  'do.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'everyone',\n",
              "  'in',\n",
              "  'the',\n",
              "  'family',\n",
              "  'has',\n",
              "  'stopped',\n",
              "  'eating',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  'and',\n",
              "  'grains.',\n",
              "  'now',\n",
              "  'all',\n",
              "  'our',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  'have',\n",
              "  '<unk>',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'tom',\n",
              "  'is',\n",
              "  'the',\n",
              "  'one',\n",
              "  \"that's\",\n",
              "  'been',\n",
              "  'stealing',\n",
              "  'our',\n",
              "  'sheep.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  \"i'm\",\n",
              "  'the',\n",
              "  'only',\n",
              "  'one',\n",
              "  'here',\n",
              "  'who',\n",
              "  \"doesn't\",\n",
              "  'speak',\n",
              "  'french.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'i',\n",
              "  'love',\n",
              "  'writing',\n",
              "  'on',\n",
              "  'yellow',\n",
              "  'or',\n",
              "  'other',\n",
              "  '<unk>',\n",
              "  'paper.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'tom',\n",
              "  'and',\n",
              "  'mary',\n",
              "  'said',\n",
              "  'that',\n",
              "  'they',\n",
              "  'wanted',\n",
              "  'to',\n",
              "  'meet',\n",
              "  'john.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'some',\n",
              "  'people',\n",
              "  'call',\n",
              "  '<unk>',\n",
              "  'an',\n",
              "  'apartheid',\n",
              "  'state.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'no',\n",
              "  'one',\n",
              "  'has',\n",
              "  'the',\n",
              "  'right',\n",
              "  'to',\n",
              "  'tell',\n",
              "  'me',\n",
              "  'what',\n",
              "  'i',\n",
              "  'can',\n",
              "  'and',\n",
              "  \"can't\",\n",
              "  'do',\n",
              "  'with',\n",
              "  'my',\n",
              "  'body.',\n",
              "  '</s>'],\n",
              " ['<s>', 'i', 'had', 'the', '<unk>', '</s>'],\n",
              " ['<s>',\n",
              "  'the',\n",
              "  'girl',\n",
              "  'who',\n",
              "  'works',\n",
              "  'at',\n",
              "  'the',\n",
              "  'bakery',\n",
              "  'is',\n",
              "  'cute.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'why',\n",
              "  'is',\n",
              "  'mary',\n",
              "  'going',\n",
              "  'with',\n",
              "  'him',\n",
              "  'to',\n",
              "  'the',\n",
              "  '<unk>',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'he',\n",
              "  'remained',\n",
              "  '<unk>',\n",
              "  'for',\n",
              "  'a',\n",
              "  'long',\n",
              "  'time,',\n",
              "  'trying',\n",
              "  'to',\n",
              "  'find',\n",
              "  'a',\n",
              "  'solution',\n",
              "  'to',\n",
              "  'the',\n",
              "  'problem.',\n",
              "  '</s>'],\n",
              " ['<s>', 'sami', '<unk>', 'the', 'bus', 'all', 'the', 'time.', '</s>'],\n",
              " ['<s>',\n",
              "  'the',\n",
              "  'success',\n",
              "  'of',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  'and',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  'may',\n",
              "  'hold',\n",
              "  'the',\n",
              "  'key',\n",
              "  'to',\n",
              "  '<unk>',\n",
              "  'this',\n",
              "  'economic',\n",
              "  '<unk>',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  \"i'll\",\n",
              "  'tell',\n",
              "  'tom',\n",
              "  'and',\n",
              "  'mary',\n",
              "  \"we're\",\n",
              "  'going',\n",
              "  'with',\n",
              "  'them.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'the',\n",
              "  'marriage',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  'and',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  'of',\n",
              "  '<unk>',\n",
              "  'now',\n",
              "  'went',\n",
              "  '<unk>',\n",
              "  'the',\n",
              "  '<unk>',\n",
              "  'showing',\n",
              "  'no',\n",
              "  '<unk>',\n",
              "  'but',\n",
              "  'being',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  'in',\n",
              "  'everything',\n",
              "  'her',\n",
              "  'mother',\n",
              "  '<unk>',\n",
              "  'or',\n",
              "  '<unk>',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'not',\n",
              "  'only',\n",
              "  'has',\n",
              "  'roger',\n",
              "  '<unk>',\n",
              "  'only',\n",
              "  'been',\n",
              "  '<unk>',\n",
              "  'once',\n",
              "  'since',\n",
              "  '<unk>',\n",
              "  '18,',\n",
              "  'but',\n",
              "  'he',\n",
              "  'has',\n",
              "  'never',\n",
              "  'been',\n",
              "  '<unk>',\n",
              "  'either.',\n",
              "  'for',\n",
              "  'someone',\n",
              "  'with',\n",
              "  'over',\n",
              "  'a',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  \"that's\",\n",
              "  'certainly',\n",
              "  'a',\n",
              "  'record.',\n",
              "  '</s>'],\n",
              " ['<s>', 'layla', 'was', 'ready', 'for', 'the', 'trip.', '</s>'],\n",
              " ['<s>',\n",
              "  'mary',\n",
              "  'said',\n",
              "  'that',\n",
              "  \"wasn't\",\n",
              "  'the',\n",
              "  'real',\n",
              "  'reason',\n",
              "  'she',\n",
              "  'did',\n",
              "  'that.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'the',\n",
              "  '<unk>',\n",
              "  'said',\n",
              "  'the',\n",
              "  'repair',\n",
              "  'would',\n",
              "  'not',\n",
              "  'take',\n",
              "  'long.',\n",
              "  '</s>'],\n",
              " ['<s>', 'they', 'pulled', 'their', '<unk>', 'off.', '</s>'],\n",
              " ['<s>',\n",
              "  'tom',\n",
              "  'and',\n",
              "  'mary',\n",
              "  'told',\n",
              "  'me',\n",
              "  'they',\n",
              "  \"couldn't\",\n",
              "  'do',\n",
              "  'that.',\n",
              "  '</s>'],\n",
              " ['<s>', 'he', 'went', 'to', 'the', 'university', 'of', '<unk>', '</s>'],\n",
              " ['<s>',\n",
              "  'he',\n",
              "  \"can't\",\n",
              "  'tell',\n",
              "  'a',\n",
              "  '<unk>',\n",
              "  'tree',\n",
              "  'from',\n",
              "  'a',\n",
              "  'plum',\n",
              "  'tree,',\n",
              "  'but',\n",
              "  'he',\n",
              "  'can',\n",
              "  'name',\n",
              "  'them',\n",
              "  'in',\n",
              "  '<unk>',\n",
              "  'languages.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'you',\n",
              "  \"couldn't\",\n",
              "  'solve',\n",
              "  'the',\n",
              "  'problem,',\n",
              "  'could',\n",
              "  'you?',\n",
              "  '</s>'],\n",
              " ['<s>', '<unk>', 'is', 'the', 'language', 'of', 'the', 'universe.', '</s>'],\n",
              " ['<s>',\n",
              "  'tom',\n",
              "  'knew',\n",
              "  'that',\n",
              "  'mary',\n",
              "  'wanted',\n",
              "  'him',\n",
              "  'to',\n",
              "  'ask',\n",
              "  'her',\n",
              "  'to',\n",
              "  'go',\n",
              "  'to',\n",
              "  'the',\n",
              "  'dance',\n",
              "  'with',\n",
              "  'him.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'tom',\n",
              "  'has',\n",
              "  'been',\n",
              "  'living',\n",
              "  'in',\n",
              "  'boston',\n",
              "  'with',\n",
              "  'his',\n",
              "  'father.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'tom',\n",
              "  'and',\n",
              "  'mary',\n",
              "  'have',\n",
              "  'lost',\n",
              "  'control',\n",
              "  'of',\n",
              "  'the',\n",
              "  'situation.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'i',\n",
              "  'want',\n",
              "  'to',\n",
              "  'be',\n",
              "  'the',\n",
              "  'best',\n",
              "  'player',\n",
              "  'i',\n",
              "  'can',\n",
              "  'be.',\n",
              "  '</s>'],\n",
              " ['<s>', 'i', 'have', 'completed', 'the', '<unk>', '</s>'],\n",
              " ['<s>', 'i', 'like', 'the', 'way', 'she', 'smiles.', '</s>'],\n",
              " ['<s>',\n",
              "  'they',\n",
              "  'said',\n",
              "  'that',\n",
              "  \"they'd\",\n",
              "  'help',\n",
              "  'us',\n",
              "  'do',\n",
              "  'that.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'those',\n",
              "  'who',\n",
              "  'have',\n",
              "  'never',\n",
              "  'thought',\n",
              "  'about',\n",
              "  'the',\n",
              "  'value',\n",
              "  'of',\n",
              "  'life',\n",
              "  'should',\n",
              "  'not',\n",
              "  'study',\n",
              "  'medicine.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'we',\n",
              "  'were',\n",
              "  'able',\n",
              "  'to',\n",
              "  'make',\n",
              "  'a',\n",
              "  'deal',\n",
              "  'with',\n",
              "  'them.',\n",
              "  '</s>'],\n",
              " ['<s>', 'will', 'you', 'please', 'pass', 'the', '<unk>', '</s>'],\n",
              " ['<s>', 'i', 'slept', 'on', 'the', 'floor.', '</s>'],\n",
              " ['<s>', 'they', 'probably', 'did', 'that', 'by', 'themselves.', '</s>'],\n",
              " ['<s>',\n",
              "  'tom',\n",
              "  'was',\n",
              "  'in',\n",
              "  'the',\n",
              "  'hospital',\n",
              "  'for',\n",
              "  'three',\n",
              "  'weeks',\n",
              "  'earlier',\n",
              "  'this',\n",
              "  'year.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'while',\n",
              "  'i',\n",
              "  'was',\n",
              "  'thinking',\n",
              "  'over',\n",
              "  'whether',\n",
              "  'i',\n",
              "  'should',\n",
              "  'accept',\n",
              "  'such',\n",
              "  'strange',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  'through',\n",
              "  'the',\n",
              "  'book',\n",
              "  'and',\n",
              "  'read',\n",
              "  '<unk>',\n",
              "  'and',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  'i',\n",
              "  'was',\n",
              "  'thinking',\n",
              "  'over',\n",
              "  'whether',\n",
              "  'i',\n",
              "  'should',\n",
              "  'accept',\n",
              "  'such',\n",
              "  'strange',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  'through',\n",
              "  'the',\n",
              "  'book',\n",
              "  'and',\n",
              "  'read',\n",
              "  '<unk>',\n",
              "  'and',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  'i',\n",
              "  'was',\n",
              "  '<unk>',\n",
              "  '<unk>',\n",
              "  'quickly',\n",
              "  '<unk>',\n",
              "  'the',\n",
              "  '<unk>',\n",
              "  'from',\n",
              "  '<unk>',\n",
              "  'hands.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'do',\n",
              "  'you',\n",
              "  'think',\n",
              "  'tom',\n",
              "  'and',\n",
              "  'mary',\n",
              "  'would',\n",
              "  'like',\n",
              "  'it',\n",
              "  'if',\n",
              "  'i',\n",
              "  'did',\n",
              "  'that',\n",
              "  'for',\n",
              "  'them?',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'tom',\n",
              "  'and',\n",
              "  'mary',\n",
              "  'cut',\n",
              "  'it',\n",
              "  'with',\n",
              "  'their',\n",
              "  'knives.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'they',\n",
              "  'danced',\n",
              "  'to',\n",
              "  'the',\n",
              "  'sound',\n",
              "  'of',\n",
              "  'the',\n",
              "  'music.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'you',\n",
              "  'always',\n",
              "  'do',\n",
              "  'that',\n",
              "  'in',\n",
              "  'the',\n",
              "  '<unk>',\n",
              "  \"don't\",\n",
              "  'you?',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'this',\n",
              "  'is',\n",
              "  'the',\n",
              "  'best',\n",
              "  'method',\n",
              "  'to',\n",
              "  'solve',\n",
              "  'that',\n",
              "  'problem.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'tom',\n",
              "  'knew',\n",
              "  'that',\n",
              "  'mary',\n",
              "  'had',\n",
              "  'told',\n",
              "  'john',\n",
              "  'that',\n",
              "  'she',\n",
              "  'saw',\n",
              "  'alice',\n",
              "  '<unk>',\n",
              "  'another',\n",
              "  'man.',\n",
              "  '</s>'],\n",
              " ['<s>',\n",
              "  'tom',\n",
              "  '<unk>',\n",
              "  'three',\n",
              "  'hundred',\n",
              "  'dollars',\n",
              "  'for',\n",
              "  'the',\n",
              "  'party.',\n",
              "  '</s>']]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FGh0-J6Iwoym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0c189cc-0af0-449d-e01a-fa29bc540c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The perplexity is 138.72070613113132\n"
          ]
        }
      ],
      "source": [
        "# COMPUTE PERPLEXITY ON VALIDATION SET\n",
        "\n",
        "print(\"The perplexity is\", perplexity(model, valid_data, n=n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7XEoMNbvwoyn"
      },
      "outputs": [],
      "source": [
        "def get_proba_distrib(model, context):\n",
        "    ## need to get the the words after the context and their probability of appearance\n",
        "    ## after this context \n",
        "    '''\n",
        "    Parameters: \n",
        "    model (dictionary of dictionary)\n",
        "    {\n",
        "        context: {word:probability of this word given context}\n",
        "    }\n",
        "    context (list of strings): the sentence we need to find the words after it and \n",
        "    thier probabilites\n",
        "    \n",
        "    Retunrs:\n",
        "    words_and_probs(dic): {word: probability of word given context}\n",
        "    \n",
        "    '''\n",
        "    # code a recursive function over context\n",
        "    # to find the longest available ngram\n",
        "    \n",
        "    ## FILL CODE\n",
        "\n",
        "    for _ in range(len(context)):\n",
        "      #check\n",
        "      map = model[tuple([context])]\n",
        "      if map == None:\n",
        "        #no match, remove pop(0)\n",
        "        context.pop(0)\n",
        "        #retry\n",
        "        continue\n",
        "      \n",
        "      #match, return nexts...\n",
        "      return map"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "['1','2','3'][-1]"
      ],
      "metadata": {
        "id": "iu-efwtiJb5O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "49a8b08c-add7-4ab9-cf66-fe9da61a919a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rr = ['1']\n",
        "for _ in range(len(rr)):\n",
        "  print(rr)\n",
        "  rr.pop(0)\n",
        "print(rr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOZ8T_l1vswu",
        "outputId": "abda44ae-351f-4671-f163-a11bcd65b21f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1']\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "KMlF6uMHwoyn"
      },
      "outputs": [],
      "source": [
        "def generate(model):\n",
        "    '''\n",
        "    Parameters: \n",
        "    model (dictionary of dictionary)\n",
        "    {\n",
        "        context: {word:probability of this word given context}\n",
        "    }\n",
        "    \n",
        "    Retunrs:\n",
        "    sentence (list of strings): a sentence sampled according to the language model. \n",
        "    \n",
        "\n",
        "    '''\n",
        "    # generate a sentence. A sentence starts with a <s> and ends with a </s>\n",
        "    # Possiblly a use function is:\n",
        "    # np.random.choice(x, 1, p = y)\n",
        "\n",
        "    # where x is a list of things to sample from\n",
        "    # and y is a list of probability (of the same length as x)\n",
        "    sentence = [\"<s>\"]\n",
        "    while sentence[-1] != \"</s>\" and len(sentence)<100:\n",
        "        context = sentence[-1]\n",
        "        next_choice_of_words = get_proba_distrib(model,context)\n",
        "        next_choice_of_words_lists = [k[0] for k in next_choice_of_words.keys()]\n",
        "        # print(next_choice_of_words_lists)\n",
        "        # print(next_choice_of_words)\n",
        "        choice = np.random.choice(next_choice_of_words_lists, 1, replace=False)\n",
        "        sentence.append(choice[0])\n",
        "        \n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "iWTH570vwoyn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b802435f-007d-4abc-9d39-ed7de41456d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>\n",
            "lady\n",
            "was\n",
            "haunted.\n",
            "Generated sentence:  ['<s>', 'lady', 'was', 'haunted.', '</s>']\n"
          ]
        }
      ],
      "source": [
        "# GENERATE A SENTENCE FROM THE MODEL\n",
        "\n",
        "\n",
        "print(\"Generated sentence: \",generate(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXY_E70mwoyn"
      },
      "source": [
        "Once you are done implementing the model, evaluation and generation code, you can try changing the value of `n`, and play with a larger training set (`train2.txt` and `valid2.txt`). You can also try to implement an interpolation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqSNFVsYwoyn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2+"
    },
    "colab": {
      "name": "Copy of n_gram_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}